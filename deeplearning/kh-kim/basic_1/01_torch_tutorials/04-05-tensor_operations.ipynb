{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic Element-wise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2]) 2\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([[1, 2],\n",
    "                       [3, 4]])\n",
    "b = torch.FloatTensor([[2, 2],\n",
    "                       [3, 3]])\n",
    "print(a.shape, a.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [6., 7.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  0.],\n",
       "        [ 0.,  1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.],\n",
       "        [ 9., 12.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 1.0000],\n",
       "        [1.0000, 1.3333]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True],\n",
       "        [ True, False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False],\n",
       "        [False,  True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a != b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.],\n",
       "        [27., 64.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inplace Operations\n",
    "\n",
    "Inplace Operation(인플레이스 연산)은 기존의 데이터를 덮어쓰면서 연산을 수행하는 작업을 의미합니다. 즉, 새로운 메모리 공간을 할당하지 않고, 기존 텐서의 값이 직접 변경됩니다. PyTorch에서는 이런 인플레이스 연산을 수행할 때 주로 연산자 뒤에 _(언더스코어)를 붙여 표현합니다.\n",
    "\n",
    "### PyTorch에서 자주 사용되는 인플레이스 연산자:\n",
    "- add_(): 덧셈\n",
    "- sub_(): 뺄셈\n",
    "- mul_(): 곱셈\n",
    "- div_(): 나눗셈\n",
    "- zero_(): 모든 값을 0으로 설정\n",
    "- fill_(): 특정 값으로 모든 요소를 채움\n",
    "\n",
    "### 인플레이스 연산의 장단점:\n",
    "- 장점: 메모리 사용을 줄일 수 있습니다. 새로운 텐서를 생성하지 않기 때문에 메모리 효율성이 증가합니다.\n",
    "- 단점: 원본 데이터가 변경되므로, 의도치 않은 데이터 손실이나 에러가 발생할 수 있습니다. 특히, 그래디언트 연산을 포함한 역전파 과정에서는 인플레이스 연산이 문제를 일으킬 수 있기 때문에 주의가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 9., 12.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 9., 12.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 9., 12.]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a.mul(b))\n",
    "print(a)\n",
    "print(a.mul_(b))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum, Mean (Dimension Reducing Operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1, 2],\n",
    "                       [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "print(x.sum())\n",
    "print(x.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim 파라미터는 PyTorch에서 텐서 연산을 수행할 때 특정 차원(dimension)을 기준으로 연산을 수행하도록 지정하는 역할을 합니다. dim 파라미터는 텐서의 어느 축(axis) 또는 차원에서 연산이 이루어져야 하는지를 결정합니다.\n",
    "\n",
    "- 차원 0 (dim=0): 행(row)에 해당하는 차원 (위에서 아래로, 수직)\n",
    "- 차원 1 (dim=1): 열(column)에 해당하는 차원 (왼쪽에서 오른쪽으로, 수평)\n",
    "- 차원 -1 (dim=-1): 마지막 차원을 의미합니다. 이 경우에는 차원 1과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 6.]) torch.Size([2])\n",
      "tensor([3., 7.]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(x.sum(dim=0), x.sum(dim=0).shape)\n",
    "print(x.sum(dim=-1), x.sum(dim=-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5차원 텐서는 (D0, D1, D2, D3, D4)의 모양을 가집니다. 예를 들어, (2, 3, 4, 2, 2) 크기의 텐서를 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-1.,  1.],\n",
       "           [-0., -0.]],\n",
       "\n",
       "          [[-1.,  1.],\n",
       "           [-1.,  1.]],\n",
       "\n",
       "          [[ 0.,  2.],\n",
       "           [-0.,  1.]],\n",
       "\n",
       "          [[ 0.,  2.],\n",
       "           [-2.,  2.]]],\n",
       "\n",
       "\n",
       "         [[[-1.,  1.],\n",
       "           [-3., -1.]],\n",
       "\n",
       "          [[-0.,  2.],\n",
       "           [-1.,  2.]],\n",
       "\n",
       "          [[ 1.,  0.],\n",
       "           [-0.,  2.]],\n",
       "\n",
       "          [[-1., -0.],\n",
       "           [ 0.,  0.]]],\n",
       "\n",
       "\n",
       "         [[[ 0.,  0.],\n",
       "           [ 0.,  2.]],\n",
       "\n",
       "          [[ 0., -1.],\n",
       "           [-1., -1.]],\n",
       "\n",
       "          [[ 2.,  1.],\n",
       "           [-1.,  1.]],\n",
       "\n",
       "          [[-1., -0.],\n",
       "           [ 1., -1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-1., -0.],\n",
       "           [-1.,  1.]],\n",
       "\n",
       "          [[ 1.,  0.],\n",
       "           [ 1., -1.]],\n",
       "\n",
       "          [[ 1., -1.],\n",
       "           [-1., -1.]],\n",
       "\n",
       "          [[ 0., -1.],\n",
       "           [-0.,  0.]]],\n",
       "\n",
       "\n",
       "         [[[-1., -1.],\n",
       "           [ 3., -1.]],\n",
       "\n",
       "          [[ 1., -1.],\n",
       "           [ 0., -1.]],\n",
       "\n",
       "          [[-0., -1.],\n",
       "           [ 0.,  1.]],\n",
       "\n",
       "          [[-1.,  2.],\n",
       "           [ 2.,  1.]]],\n",
       "\n",
       "\n",
       "         [[[ 0.,  0.],\n",
       "           [-1., -2.]],\n",
       "\n",
       "          [[-1., -2.],\n",
       "           [ 1.,  0.]],\n",
       "\n",
       "          [[-0.,  0.],\n",
       "           [-1.,  0.]],\n",
       "\n",
       "          [[-0.,  0.],\n",
       "           [-0.,  3.]]]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.round(torch.randn(2, 3, 4, 2, 2))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dim=0: 첫 번째 차원을 따라 합산합니다. (D1, D2, D3, D4) 모양의 텐서가 됩니다.\n",
    "- dim=1: 두 번째 차원을 따라 합산합니다. (D0, D2, D3, D4) 모양의 텐서가 됩니다.\n",
    "- dim=2: 세 번째 차원을 따라 합산합니다. (D0, D1, D3, D4) 모양의 텐서가 됩니다.\n",
    "- dim=-1: 마지막 차원을 따라 합산합니다. (D0, D1, D2, D3) 모양의 텐서가 됩니다.\n",
    "\n",
    "각 차원에서의 합산은 해당 차원에 위치한 요소들을 합하여 그 차원을 제거하는 방식으로 이루어지며, 나머지 차원은 그대로 유지됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor([[[[-2.,  1.],\n",
      "          [-1.,  1.]],\n",
      "\n",
      "         [[ 0.,  1.],\n",
      "          [ 0.,  0.]],\n",
      "\n",
      "         [[ 1.,  1.],\n",
      "          [-1.,  0.]],\n",
      "\n",
      "         [[ 0.,  1.],\n",
      "          [-2.,  2.]]],\n",
      "\n",
      "\n",
      "        [[[-2.,  0.],\n",
      "          [ 0., -2.]],\n",
      "\n",
      "         [[ 1.,  1.],\n",
      "          [-1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.],\n",
      "          [ 0.,  3.]],\n",
      "\n",
      "         [[-2.,  2.],\n",
      "          [ 2.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.],\n",
      "          [-1.,  0.]],\n",
      "\n",
      "         [[-1., -3.],\n",
      "          [ 0., -1.]],\n",
      "\n",
      "         [[ 2.,  1.],\n",
      "          [-2.,  1.]],\n",
      "\n",
      "         [[-1.,  0.],\n",
      "          [ 1.,  2.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(x.sum())\n",
    "print(x.sum(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast in Operations\n",
    "\n",
    "\n",
    "Tensor의 연산에서 같은 모양(Shape)을 가지는 텐서끼리의 연산이 기본적으로 가장 직관적이고 일반적입니다. 그러나 PyTorch와 같은 라이브러리에서는 모양이 다르더라도 특정 조건을 만족하면 연산이 가능합니다. 이를 **브로드캐스팅(Broadcasting)**이라고 합니다.\n",
    "\n",
    "- 두 텐서의 모양이 다를 때, 특정 규칙에 따라 작은 텐서의 모양이 자동으로 큰 텐서와 맞춰진 후 연산이 수행됩니다.\n",
    "### 브로드캐스팅 규칙:\n",
    "- 뒤에서부터(오른쪽에서부터) 차원을 비교하며, 각 차원에서 모양이 같거나 하나의 차원이 1이면 연산이 가능합니다.\n",
    "- 차원이 1인 텐서는 자동으로 반복되어(복제되어) 큰 텐서의 차원에 맞춰집니다\n",
    "\n",
    "\n",
    "What we did before,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2]])\n",
    "y = torch.FloatTensor([[4, 8]])\n",
    "\n",
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5., 10.]])\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast feature provides operations between different shape of tensors.\n",
    "\n",
    "### Tensor + Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2],\n",
    "                       [3, 4]])\n",
    "y = 1\n",
    "\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3.],\n",
      "        [4., 5.]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor + Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2],\n",
    "                       [4, 8]])\n",
    "y = torch.FloatTensor([3,\n",
    "                       5])\n",
    "\n",
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  7.],\n",
      "        [ 7., 13.]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2]) 3\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[[1, 2]]])\n",
    "y = torch.FloatTensor([3,\n",
    "                       5])\n",
    "\n",
    "print(x.size(), x.dim())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[4., 7.]]])\n",
      "torch.Size([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor + Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2]])\n",
    "y = torch.FloatTensor([[3],\n",
    "                       [5]])\n",
    "\n",
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.],\n",
      "        [6., 7.]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need to be careful before using broadcast feature.\n",
    "\n",
    "### Failure Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2])\n",
      "torch.Size([3, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d79203316342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[[1, 2],\n",
    "                        [4, 8]]])\n",
    "y = torch.FloatTensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "\n",
    "print(x.size())\n",
    "print(y.size())\n",
    "\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
