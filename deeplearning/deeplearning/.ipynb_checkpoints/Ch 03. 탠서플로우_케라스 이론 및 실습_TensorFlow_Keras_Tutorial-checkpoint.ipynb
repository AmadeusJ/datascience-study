{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aauKER9dYuDM"
   },
   "source": [
    "# TensorFlow / Keras\n",
    "\n",
    "TensorFlow와 Keras는 ML 모델을 개발하고 학습시키는 데 도움이 되는 핵심 오픈소스 라이브러리입니다.\n",
    "\n",
    "TensorFlow와 Keras는 모두 2015년에 릴리즈 되었습니다 (Keras는 2015년 3월, TensorFlow는 2015년 11월). 이는 딥러닝 세계의 관점에서 볼 때, 꽤 오랜시간이라고 볼 수 있습니다.\n",
    "\n",
    "Keras는 사용자가 TensorFlow를 좀 더 쉽고 편하게 사용할 수 있게 해주는 high level API를 제공합니다.\n",
    "\n",
    "TensorFlow 2.x에서는 Keras를 딥러닝의 공식 API로 채택하였고, Keras는 TensorFlow 내의 하나의 framwork으로 개발되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMv8P8IDZuY9"
   },
   "source": [
    "## TensorFlow / Keras Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7ntEsFpZxd8"
   },
   "source": [
    "### TensorFlow / Keras import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JeRr6dfwVeER"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Vl0o3w_dZ_4S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n",
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHxt15A3aHUz"
   },
   "source": [
    "### TensorFlow / Keras 맛보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "U4XmcMbyaCQb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IZZ-rbdaRLE"
   },
   "outputs": [],
   "source": [
    "# MNIST dataset download\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30LQ5WRQaVAj"
   },
   "outputs": [],
   "source": [
    "# Model 생성, compile\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpbIkSwUabLD"
   },
   "outputs": [],
   "source": [
    "# Training / Evaluation\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "model.evaluate(x_test,  y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCNJ6P1earGM"
   },
   "source": [
    "데이터를 탐색해봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGXMimvQagBD"
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(x_train))\n",
    "image = x_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gh_0tCMUawqM"
   },
   "outputs": [],
   "source": [
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(y_train[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hAxc-G9a3GF"
   },
   "source": [
    "내가 쓴 손글씨로 Test 해봅시다.\n",
    "\n",
    "Colab을 쓰는 경우에는 아래 cell을 실행하면 파일을 업로드할 수 있습니다.\n",
    "\n",
    "그림판과 같은 도구를 이용하여 손으로 숫자를 쓴 다음 파일로 저장하고 업로드 합니다.\n",
    "\n",
    "이 때 파일명은 image.png로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zbq6hh6Saxt0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puSKAGeYa6Zz"
   },
   "outputs": [],
   "source": [
    "# image file의 경로 설정\n",
    "cur_dir = os.getcwd()\n",
    "img_path = os.path.join(cur_dir, 'image.png')\n",
    "# image file 읽기\n",
    "cur_img = Image.open(img_path)\n",
    "# 28x28로 resize\n",
    "cur_img = cur_img.resize((28, 28))\n",
    "image = np.asarray(cur_img)\n",
    "\n",
    "# color image일 경우 RGB 평균값으로 gray scale로 변경\n",
    "try:\n",
    "  image = np.mean(image, axis=2)\n",
    "except:\n",
    "  pass\n",
    "# upload한 image는 흰 배경에 검은 글씨로 되어 있으므로, MNIST data와 같이 검은 배경에 흰 글씨로 변경\n",
    "image = np.abs(255-image)\n",
    "# MNIST와 동일하게 data preprocessing(255로 나눠줌)\n",
    "image = image.astype(np.float32)/255.\n",
    "# 화면에 출력하여 확인\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YS--auCAa9SL"
   },
   "outputs": [],
   "source": [
    "# shape을 변경하여 학습된 model에 넣고 결과 확인\n",
    "image = np.reshape(image, (1, 28, 28))\n",
    "print(model.predict(image))\n",
    "\n",
    "print(\"Model이 예측한 값은 {} 입니다.\".format(np.argmax(model.predict(image), -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wU7Rf-PbbOk8"
   },
   "source": [
    "### Tensor\n",
    "\n",
    "Tensor는 multi-dimensional array를 나타내는 말로, TensorFlow의 기본 data type입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YfJJpCcybAhj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3. 3.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Hello World\n",
    "hello = tf.constant([3,3], dtype=tf.float32)\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HchAr_B6bYWz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "# 상수형 tensor는 아래와 같이 만들 수 있습니다\n",
    "# 출력해보면 tensor의 값과 함께, shape과 내부의 data type을 함께 볼 수 있습니다\n",
    "x = tf.constant([[1.0, 2.0],\n",
    "                 [3.0, 4.0]])\n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "j3SNfH7hbbZs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 아래와 같이 numpy ndarray나 python의 list도 tensor로 바꿀 수 있습니다\n",
    "x_np = np.array([[1.0, 2.0],\n",
    "                [3.0, 4.0]])\n",
    "x_list = [[1.0, 2.0], \n",
    "         [3.0, 4.0]]\n",
    "\n",
    "print(type(x_np))\n",
    "print(type(x_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LXT6RTiObcft"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "x_np = tf.convert_to_tensor(x_np)\n",
    "x_list = tf.convert_to_tensor(x_list)\n",
    "\n",
    "print(type(x_np))\n",
    "print(type(x_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MIG2-h_8begE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 반대로 tensor를 다음과 같이 numpy ndarray로 바꿀 수도 있습니다\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qAwnWTrfbfhk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0SwhFw8DbgxM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2 2]\n",
      " [2 2]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones((2,3))\n",
    "b = tf.zeros((2,3))\n",
    "c = tf.fill((2,2), 2)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AYp8GW02b-NT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0]\n",
      " [0 0]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 1]\n",
      " [1 1]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "d = tf.zeros_like(c)\n",
    "e = tf.ones_like(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QjkKxrB6ccsc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "f = tf.eye(3)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hA2msfJDlVMm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "g  = tf.range(10)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cSDPGRvalgBu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.80563414 0.37865102]\n",
      " [0.51143265 0.1132468 ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.05838714  2.4181526 ]\n",
      " [ 0.4858334  -0.17025635]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "h = tf.random.uniform((2,2)) # np.rand\n",
    "i = tf.random.normal((2,2))  # np.randn\n",
    "print(h)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o364b_dtmCLS"
   },
   "source": [
    "#### Tensor의 속성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kJaZ9nmNlp4m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: (3, 4)\n",
      "Datatype of tensor: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "tensor = tf.random.normal((3,4))\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3RpWD9D7omzi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: (4, 3)\n",
      "Datatype of tensor: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "# 속성 변경\n",
    "tensor = tf.reshape(tensor, (4, 3))\n",
    "tensor = tf.cast(tensor, tf.int32)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRIucbyummIi"
   },
   "source": [
    "### Variable\n",
    "\n",
    "Variable은 변할 수 있는 상태를 저장하는데 사용되는 특별한 텐서입니다.\n",
    "딥러닝에서는 학습해야하는 가중치(weight, bias)들을 variable로 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Bg4SIFVImP0w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensor)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# tensor의 값 변경 - 변경 불가능\n",
    "tensor = tf.ones((3,4))\n",
    "print(tensor)\n",
    "\n",
    "tensor[0,0] = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "18QWNtDqnJzh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n",
      "array([[1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n",
      "array([[2., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# variable 만들기, 값 변경\n",
    "variable = tf.Variable(tensor)\n",
    "print(variable)\n",
    "\n",
    "variable[0,0].assign(2)\n",
    "print(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQPHxINYm_1H"
   },
   "outputs": [],
   "source": [
    "# 초기값을 사용해서 Variable을 생성할 수 있습니다\n",
    "initial_value = tf.random.normal(shape=(2, 2))\n",
    "weight = tf.Variable(initial_value)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9bbWyNQnpz3"
   },
   "outputs": [],
   "source": [
    "# 아래와 같이 variable을 초기화해주는 initializer들을 사용할 수도 있습니다\n",
    "weight = tf.Variable(tf.random_normal_initializer(stddev=1.)(shape=(2,2)))\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HmxkQVtoPFf"
   },
   "outputs": [],
   "source": [
    "# variable은 `.assign(value)`, `.assign_add(increment)`, 또는 `.assign_sub(decrement)`\n",
    "# 와 같은 메소드를 사용해서 Variable의 값을 갱신합니다:'''\n",
    "\n",
    "new_value = tf.random.normal(shape=(2,2))\n",
    "print(new_value)\n",
    "weight.assign(new_value)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a03m0sVuoRxH"
   },
   "outputs": [],
   "source": [
    "added_value = tf.ones(shape=(2,2))\n",
    "weight.assign_sub(added_value)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVvv2ITIobTs"
   },
   "source": [
    "### Indexing과 Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmDw1IOnoTtH"
   },
   "outputs": [],
   "source": [
    "a = tf.range(1, 13)\n",
    "a = tf.reshape(a, (3, 4))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HRF-l-eoh_o"
   },
   "outputs": [],
   "source": [
    "# indexing\n",
    "print(a[1])\n",
    "print(a[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgLmdhMmo2fX"
   },
   "outputs": [],
   "source": [
    "# slicing\n",
    "print(a[1:-1])\n",
    "print(a[:2, 2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X54tCulGo7xb"
   },
   "source": [
    "### Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "z1npXR75o554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]]\n",
      "\n",
      " [[ 8  9 10 11]\n",
      "  [12 13 14 15]]], shape=(2, 2, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.range(16)\n",
    "a = tf.reshape(a, (2, 2, -1))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xActsV92pVIQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  4]\n",
      "  [ 8 12]]\n",
      "\n",
      " [[ 1  5]\n",
      "  [ 9 13]]\n",
      "\n",
      " [[ 2  6]\n",
      "  [10 14]]\n",
      "\n",
      " [[ 3  7]\n",
      "  [11 15]]], shape=(4, 2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "b = tf.transpose(a, (2, 0, 1))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBKEyPlypgAz"
   },
   "source": [
    "### Tensor 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Q3c7g2JSpYPg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[5. 6.]\n",
      " [7. 8.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1,2], [3,4]], dtype=tf.float32)\n",
    "y = tf.constant([[5,6], [7,8]], dtype=tf.float32)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "FUfznMmjpmnA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 6.  8.]\n",
      " [10. 12.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-4. -4.]\n",
      " [-4. -4.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 5. 12.]\n",
      " [21. 32.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.2        0.33333334]\n",
      " [0.42857143 0.5       ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[19. 22.]\n",
      " [43. 50.]], shape=(2, 2), dtype=float32)\n",
      "==============================\n",
      "tf.Tensor(\n",
      "[[ 6.  8.]\n",
      " [10. 12.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-4. -4.]\n",
      " [-4. -4.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 5. 12.]\n",
      " [21. 32.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.2        0.33333334]\n",
      " [0.42857143 0.5       ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[19. 22.]\n",
      " [43. 50.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(x + y)\n",
    "print(x - y)\n",
    "print(x * y)\n",
    "print(x / y)\n",
    "print(x @ y)\n",
    "print('='*30)\n",
    "print(tf.add(x, y))\n",
    "print(tf.subtract(x, y))\n",
    "print(tf.multiply(x, y))\n",
    "print(tf.divide(x, y))\n",
    "print(tf.matmul(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vGwjOHyOprW4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]], shape=(2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "z = tf.range(1, 11)\n",
    "z = tf.reshape(z, (2, 5))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "XFfJw1jdSsD9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(55, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_sum(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "cr_2RM0Up6to"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  9 11 13 15], shape=(5,), dtype=int32)\n",
      "tf.Tensor([15 40], shape=(2,), dtype=int32)\n",
      "tf.Tensor([15 40], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sum1 = tf.reduce_sum(z, axis=0)\n",
    "sum2 = tf.reduce_sum(z, axis=1)\n",
    "sum3 = tf.reduce_sum(z, axis=-1)\n",
    "print(sum1)\n",
    "print(sum2)\n",
    "print(sum3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ujBDhmPcqAEg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]\n",
      " [ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]], shape=(4, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "concat = tf.concat([z, z], axis=0)\n",
    "print(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Zo7teoSJqPVw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1  2  3  4  5  1  2  3  4  5]\n",
      " [ 6  7  8  9 10  6  7  8  9 10]], shape=(2, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "concat = tf.concat([z, z], axis=-1)\n",
    "print(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "NjqZ8G3DqTv4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1  2  3  4  5]\n",
      "  [ 6  7  8  9 10]]\n",
      "\n",
      " [[ 1  2  3  4  5]\n",
      "  [ 6  7  8  9 10]]], shape=(2, 2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "stack = tf.stack([z, z], axis=0)\n",
    "print(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "elWjpDZGqXuY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1  1]\n",
      "  [ 2  2]\n",
      "  [ 3  3]\n",
      "  [ 4  4]\n",
      "  [ 5  5]]\n",
      "\n",
      " [[ 6  6]\n",
      "  [ 7  7]\n",
      "  [ 8  8]\n",
      "  [ 9  9]\n",
      "  [10 10]]], shape=(2, 5, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "stack = tf.stack([z, z], axis=-1)\n",
    "print(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Z0MJo_ObTEN9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1  2  3  4  5]\n",
      "  [ 1  2  3  4  5]]\n",
      "\n",
      " [[ 6  7  8  9 10]\n",
      "  [ 6  7  8  9 10]]], shape=(2, 2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "stack = tf.stack([z, z], axis=1)\n",
    "print(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rwqkPk6ufp3"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Data를 처리하여 model에 공급하기 위하여 TensorFlow에서는 tf.data.Dataset을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YzYF_FUuwYt"
   },
   "source": [
    "### FashoinMNIST data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "f-qiNiQ8qZ8A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.fashion_mnist\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "zPy0L1v-u7VZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# train_images, train_labels의 shape 확인\n",
    "print(train_images.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "DelbP1XivAyJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# test_images, test_labels의 shape 확인\n",
    "print(test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "D6InVziDTw_-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "y8Ojc9i9vDc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 6000,\n",
       " 1: 6000,\n",
       " 2: 6000,\n",
       " 3: 6000,\n",
       " 4: 6000,\n",
       " 5: 6000,\n",
       " 6: 6000,\n",
       " 7: 6000,\n",
       " 8: 6000,\n",
       " 9: 6000}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set의 각 class 별 image 수 확인\n",
    "unique, counts = np.unique(train_labels, axis=-1, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "f4Ff24ELvFzR",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1000,\n",
       " 1: 1000,\n",
       " 2: 1000,\n",
       " 3: 1000,\n",
       " 4: 1000,\n",
       " 5: 1000,\n",
       " 6: 1000,\n",
       " 7: 1000,\n",
       " 8: 1000,\n",
       " 9: 1000}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set의 각 class 별 image 수 확인\n",
    "unique, counts = np.unique(test_labels, axis=-1, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrWCNyPZvPgN"
   },
   "source": [
    "### Data 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "JMKlUaKfvLQJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap='gray')\n",
    "    plt.title(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7P6_DBZPvrMu"
   },
   "source": [
    "### Data 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "wfjS_-y4vqQm"
   },
   "outputs": [],
   "source": [
    "# image를 0~1사이 값으로 만들기 위하여 255로 나누어줌 (이미지는 0 ~ 255 pixel로 되어있는데, 255로 나눠주면 0~14로 => 이정도 범위에 있어야 학습이 잘된다.)\n",
    "train_images = train_images.astype(np.float32) / 255.\n",
    "test_images = test_images.astype(np.float32) / 255.\n",
    "\n",
    "# one-hot encoding => 정답만 1이고 나머지는 0 되도록 하는 것\n",
    "train_labels = keras.utils.to_categorical(train_labels, 10)\n",
    "test_labels = keras.utils.to_categorical(test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDe8mN1-vmGh"
   },
   "source": [
    "### Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "rc8hRxHWvUqZ"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.47 GiB for an array with shape (60000, 10, 10, 10, 10) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshuffle(\n\u001b[0;32m      2\u001b[0m                 buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      3\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((test_images, test_labels))\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m64\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:826\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[1;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:134\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    131\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    133\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 134\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[0;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[1;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[0;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[0;32m    293\u001b[0m )\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[0;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\deeplearning-tyeo19Ry-py3.11\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:96\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     98\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.47 GiB for an array with shape (60000, 10, 10, 10, 10) and data type float64"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(\n",
    "                buffer_size=100000).batch(64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpaQhxmvv7_a"
   },
   "outputs": [],
   "source": [
    "# Dataset을 통해 반복하기(iterate)\n",
    "# 이미지와 정답(label)을 표시합니다.\n",
    "imgs, lbs = next(iter(train_dataset))\n",
    "print(f\"Feature batch shape: {imgs.shape}\")\n",
    "print(f\"Labels batch shape: {lbs.shape}\")\n",
    "\n",
    "img = imgs[0]\n",
    "lb = lbs[0]\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(f\"Label: {lb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyrVNM2fwn12"
   },
   "source": [
    "### Custom Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzzXDgNGwnBV"
   },
   "outputs": [],
   "source": [
    "a = np.arange(10)\n",
    "print(a)\n",
    "\n",
    "ds_tensors = tf.data.Dataset.from_tensor_slices(a)\n",
    "print(ds_tensors)\n",
    "\n",
    "for x in ds_tensors:\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hAXE-CUwPmq"
   },
   "outputs": [],
   "source": [
    "# data 전처리(변환), shuffle, batch 추가\n",
    "ds_tensors = ds_tensors.map(tf.square).shuffle(10).batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbB3APdIw8QK"
   },
   "outputs": [],
   "source": [
    "# 3 epoch 처럼 돈다\n",
    "\n",
    "for _ in range(3):\n",
    "    for x in ds_tensors:\n",
    "        print(x)\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ek-Z53ZpxD7P"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aFljiFtxGUP"
   },
   "source": [
    "### Keras Sequential API 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Id8VOEiw-nT"
   },
   "outputs": [],
   "source": [
    "def create_seq_model():\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "  model.add(keras.layers.Dense(128, activation='relu'))\n",
    "  model.add(keras.layers.Dropout(0.2))\n",
    "  model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc-0GKtqxopy"
   },
   "outputs": [],
   "source": [
    "seq_model = create_seq_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVask4mlxtLy"
   },
   "outputs": [],
   "source": [
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fB29iRAxyP_"
   },
   "source": [
    "### Keras Functional API 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzzxJsKfxwXS"
   },
   "outputs": [],
   "source": [
    "def create_func_model():\n",
    "  inputs = keras.Input(shape=(28,28))\n",
    "  flatten = keras.layers.Flatten()(inputs)\n",
    "  dense = keras.layers.Dense(128, activation='relu')(flatten)\n",
    "  drop = keras.layers.Dropout(0.2)(dense)\n",
    "  outputs = keras.layers.Dense(10, activation='softmax')(drop)\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvFHBdqyyhJ6"
   },
   "outputs": [],
   "source": [
    "func_model = create_func_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYqg6n73yjXq"
   },
   "outputs": [],
   "source": [
    "func_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3weqVyOymnn"
   },
   "source": [
    "### Model Class Subclassing 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8pq4dLnykdS"
   },
   "outputs": [],
   "source": [
    "class SubClassModel(keras.Model):\n",
    "  def __init__(self):\n",
    "    super(SubClassModel, self).__init__()\n",
    "    self.flatten = keras.layers.Flatten(input_shape=(28, 28))\n",
    "    self.dense1 = keras.layers.Dense(128, activation='relu')\n",
    "    self.drop = keras.layers.Dropout(0.2)\n",
    "    self.dense2 = keras.layers.Dense(10, activation='softmax')\n",
    "  def call(self, x, training=False):\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense1(x)\n",
    "    x = self.drop(x)\n",
    "    return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tt52E2NWzURC"
   },
   "outputs": [],
   "source": [
    "subclass_model = SubClassModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV_KgXakzadz"
   },
   "outputs": [],
   "source": [
    "inputs = tf.zeros((1, 28, 28))\n",
    "subclass_model(inputs)\n",
    "subclass_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYl4PqbCzbtz"
   },
   "outputs": [],
   "source": [
    "# 가상의 data 만들어서 예측해보기\n",
    "inputs = tf.random.normal((1, 28, 28))\n",
    "outputs = subclass_model(inputs)\n",
    "pred = tf.argmax(outputs, -1)\n",
    "print(f\"Predicted class: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjgp8OAD01GH"
   },
   "source": [
    "## Traning / Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJlGY-dy5J5q"
   },
   "source": [
    "### Keras API 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6cF-y6R0yCL"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "seq_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dioyr2Q057wc"
   },
   "outputs": [],
   "source": [
    "history = seq_model.fit(train_dataset, epochs=10, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tilZI3YW6Tll"
   },
   "outputs": [],
   "source": [
    "## Plot losses\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LY-a3Y546xhs"
   },
   "outputs": [],
   "source": [
    "## Plot Accuracy\n",
    "plt.plot(history.history['accuracy'], 'b-', label='acc')\n",
    "plt.plot(history.history['val_accuracy'], 'r--', label='val_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VFqWUCm7K_z"
   },
   "source": [
    "### GradientTape 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOwN4DFI60R9"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_object = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "153jt421803l"
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBDHMbMD9-nW"
   },
   "outputs": [],
   "source": [
    "# loss, accuracy 계산\n",
    "train_loss = keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = keras.metrics.CategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tc9TNfY7-QI9"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=True)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnnvO_O4-0m-"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(model, images, labels):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  predictions = model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukQTFQ3C-4H2"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  for images, labels in train_dataset:\n",
    "    train_step(func_model, images, labels)\n",
    "\n",
    "  for test_images, test_labels in test_dataset:\n",
    "    test_step(func_model, test_images, test_labels)\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsylMBvG_SKk"
   },
   "source": [
    "## Model 저장하고 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILZ2UZtG_Wcs"
   },
   "source": [
    "### parameter만 저장하고 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aW3urlf_--wV"
   },
   "outputs": [],
   "source": [
    "seq_model.save_weights('seq_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jscos7SR_rne"
   },
   "outputs": [],
   "source": [
    "seq_model_2 = create_seq_model()\n",
    "seq_model_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG5_n34L_w-m"
   },
   "outputs": [],
   "source": [
    "seq_model_2.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cG35uDOz_z-u"
   },
   "outputs": [],
   "source": [
    "seq_model_2.load_weights('seq_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RY0CZ78kAFmW"
   },
   "outputs": [],
   "source": [
    "seq_model_2.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u78ZB3NmAKIj"
   },
   "source": [
    "### Model 전체를 저장하고 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwIHNJj4AICO"
   },
   "outputs": [],
   "source": [
    "seq_model.save('seq_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjEAsrR8ARgm"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpWXRoL6ASPG"
   },
   "outputs": [],
   "source": [
    "seq_model_3 = keras.models.load_model('seq_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhqSNOwYAWhY"
   },
   "outputs": [],
   "source": [
    "seq_model_3.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnC96I2rBbX2"
   },
   "source": [
    "## Tensorboard 사용하여 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-ex0q2wBozU"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9JUuJeEBemF"
   },
   "source": [
    "### Keras Callback 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbRabVS3Ahmm"
   },
   "outputs": [],
   "source": [
    "new_model_1 = create_seq_model()\n",
    "new_model_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ltOogSpB9pX"
   },
   "outputs": [],
   "source": [
    "new_model_1.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-xKwk0YB_bv"
   },
   "outputs": [],
   "source": [
    "log_dir = './logs/new_model_1'\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fpClL_kCFJ3"
   },
   "outputs": [],
   "source": [
    "new_model_1.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset,\n",
    "          callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2ax9hr6CRn2"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj3kL_VkDHGX"
   },
   "source": [
    "### Summary Writer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixN0sIh4Cdwu"
   },
   "outputs": [],
   "source": [
    "new_model_2 = create_seq_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gduevEpWFP-j"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_object = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7CDR7-vFP-k"
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hv4GjjlOFP-k"
   },
   "outputs": [],
   "source": [
    "# loss, accuracy 계산\n",
    "train_loss = keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = keras.metrics.CategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4m9Y_gRjFP-k"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=True)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO6G7COHFP-l"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(model, images, labels):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  predictions = model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2acmbCz3Dprf"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skAzBMSxEfcP"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  for images, labels in train_dataset:\n",
    "    train_step(new_model_2, images, labels)\n",
    "  with train_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "  for test_images, test_labels in test_dataset:\n",
    "    test_step(new_model_2, test_images, test_labels)\n",
    "  with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsqzb5FlEQeX"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir 'logs/gradient_tape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSbtb9aAFajQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOTCxwylZ7gH0xzmOOV86It",
   "collapsed_sections": [],
   "name": "TensorFlow_Keras_Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
